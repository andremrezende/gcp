<!DOCTYPE html>
<html lang="en-US">
   <meta http-equiv="content-type" content="text/html;charset=UTF-8" />
   <head>
      <meta charset="utf-8">
      <!--[if IE]>
      <meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
      <![endif]-->
      <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
      <link rel="profile" href="http://gmpg.org/xfn/11"/>
      <link rel="pingback" href=""/>
      <meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1'/>
      <link media="all" href="../wp-content/cache/autoptimize/css/autoptimize_65418fb62dab720bff9865a0c10b4e2d.css" rel="stylesheet"/>
      <title>Google Cloud Professional Architect Practice Exam Part 4</title>
      <meta name="description" content="Google Cloud Professional Architect Associate Practice Exam"/>
      <link rel="canonical" href="index.html"/>
      <meta property="og:locale" content="en_US"/>
      <meta property="og:type" content="article"/>
      <meta property="og:title" content="Google Cloud Professional Architect"/>
      <meta property="og:description" content="Google Cloud Professional Architect Practice Exam"/>
      <meta property="og:url" content="index.html"/>
      <meta property="og:image:width" content="1455"/>
      <meta property="og:image:height" content="818"/>
      <link rel='stylesheet' id='dashicons-css' href='../wp-includes/css/dashicons.min9f31.css?ver=5.7.2' type='text/css' media='all'/>
      <link rel='stylesheet' id='bsf-Defaults-css' href='../wp-content/cache/autoptimize/css/autoptimize_single_36ea4805809e6b690c2f5126a08082979f31.css?ver=5.7.2' type='text/css' media='all'/>
      <link rel='stylesheet' id='porto-google-fonts-css' href='http://fonts.googleapis.com/css?family=Open+Sans%3A300%2C300italic%2C400%2C400italic%2C600%2C600italic%2C700%2C700italic%2C800%2C800italic%7CShadows+Into+Light%3A300%2C300italic%2C400%2C400italic%2C600%2C600italic%2C700%2C700italic%2C800%2C800italic%7C&amp;ver=5.7.2' type='text/css' media='all'/>
      <!--[if lt IE 10]>
      <link rel='stylesheet' id='porto-ie-css'  href='https://www.awslagi.com/wp-content/themes/porto/css/ie.css?ver=5.7.2' type='text/css' media='all' />
      <![endif]--> 
   </head>
   <body class="post-template-default single single-post postid-14333 single-format-standard full blog-1 wp-schema-pro-1.5.1 wpb-js-composer js-comp-ver-4.11.2.1 vc_responsive">
      <div class="page-wrapper">
         <section class="page-top page-header-1">
            <div class="container">
               <div class="row">
                  <div class="col-md-12">
                     <div class=" hide">
                        <h1 class="page-title">Google Cloud Professional Architect Exam Part 4</h1>
                     </div>
                  </div>
               </div>
            </div>
         </section>
         <div id="main" class="column2 column2-right-sidebar boxed">
            <div class="container">
               <div class="row">
                  <div class="main-content col-md-9">
                     <div id="content" role="main">
                        <article class="post post-large post-14333 post type-post status-publish format-standard has-post-thumbnail hentry category-aws-practice-questions tag-aws-developer-associate-practice-questions">
                           <div class="post-image single">
                              <div class="post-slideshow owl-carousel">
                                 <div>
                                    <div class="img-thumbnail">
                                       <div class="inner"> <img class="owl-lazy img-responsive" width="1140" height="445" data-src="https://media.awslagi.com/wp-content/uploads/2021/08/03153733/GCP-PROACHITECT-1.jpeg" alt="Google Cloud Professional Architect" data-image="https://media.awslagi.com/2020/12/18152631/AWS-Certified-Developer-Associate-Practice-Exam.jpg" data-caption=""/> <span class="zoom"><i class="fa fa-search"></i></span></div>
                                    </div>
                                 </div>
                              </div>
                           </div>
                           <div class="post-date"> <span class="day">20</span> <span class="month">Aug</span></div>
                           <div class="post-content">
                              <h2 class="entry-title">Google Cloud Professional Architect Exam Part 4</h2>
                              <div class="entry-content">
											<p>151. Your company has a support ticketing solution that uses App Engine Standard. The project that contains the App Engine application already has a Virtual Private
                                    Cloud (VPC) network fully connected to the company's on-premises environment through a Cloud VPN tunnel. You want to enable the App Engine application to communicate with a database that is running in the company's on-premises environment. What should you do?
                                 </p>
                                 <p>A. Configure private Google access for on-premises hosts only.<br/> 
                                    B. Configure private Google access.<br/> 
                                    C. Configure private services access.<br/>
                                    D. Configure serverless VPC access.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>152. Your company is planning to upload several important files to Cloud Storage. After the upload is completed, they want to verify 
                                    that the uploaded content is identical to what they have on-premises. You want to minimize the cost and effort of performing this check. What should you do?</p>
                                 <p>A. 1. Use Linux shasum to compute a digest of files you want to upload. 2. Use gsutil -m to upload all the files to Cloud Storage. 3. Use gsutil cp to download the uploaded files. 4. Use Linux shasum to compute a digest of the downloaded files. 5. Compare the hashes.<br/> 
                                    B. 1. Use gsutil -m to upload the files to Cloud Storage. 2. Develop a custom Java application that computes CRC32C hashes. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes.<br/> 
                                    C. 1. Use gsutil -m to upload all the files to Cloud Storage. 2. Use gsutil cp to download the uploaded files. 3. Use Linux diff to compare the content of the files.<br/>
                                    D. 1. Use gsutil -m to upload the files to Cloud Storage. 2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all on-premises files. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>153. You have deployed an application on Anthos clusters (formerly Anthos GKE). According to the SRE practices at your company, you need to be alerted
                                     if request latency is above a certain threshold for a specified amount of time. What should you do?</p>
                                 <p>A. Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO), and create an alerting policy based on this SLO.<br/> 
                                    B. Enable the Cloud Trace API on your project, and use Cloud Monitoring Alerts to send an alert based on the Cloud Trace metrics.<br/> 
                                    C. Use Cloud Profiler to follow up the request latency. Create a custom metric in Cloud Monitoring based on the results of Cloud Profiler, and create an Alerting policy in case this metric exceeds the threshold.<br/>
                                    D. Configure Anthos Config Management on your cluster, and create a yaml file that defines the SLO and alerting policy you want to deploy in your cluster.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>154. Your company has a stateless web API that performs scientific calculations. The web API runs on a single Google Kubernetes Engine (GKE) cluster. The cluster is currently deployed in us-central1. Your company has expanded to offer your API to customers in Asia. You want to reduce the latency for users in Asia.
                                    What should you do?</p>
                                 <p>A. Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the Cloud DNS zone.<br/> 
                                    B. Use a global HTTP(s) load balancer with Cloud CDN enabled.<br/> 
                                    C. Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer.<br/>
                                    D. Increase the memory and CPU allocated to the application in the cluster.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>155. You are migrating third-party applications from optimized on-premises virtual machines to Google Cloud. You are unsure about the optimum CPU and memory options. 
                                    The applications have a consistent usage pattern across multiple weeks. You want to optimize resource usage for the lowest cost. What should you do?</p>
                                 <p>A. Create an instance template with the smallest available machine type, and use an image of the third-party application taken from a current on-premises virtual machine. Create a managed instance group that uses average CPU utilization to autoscale 
                                    the number of instances in the group. Modify the average CPU utilization threshold to optimize the number of instances running.<br/> 
                                    B. Create an App Engine flexible environment, and deploy the third-party application using a Dockerfile and a custom runtime.
                                     Set CPU and memory options similar to your application's current on-premises virtual machine in the app.yaml file.<br/> 
                                    C. Create multiple Compute Engine instances with varying CPU and memory options. Install the Cloud Monitoring agent, and deploy the third-party application on each of them. Run a load test with high traffic levels on the application, and use the results to determine the optimal settings.<br/>
                                    D. Create a Compute Engine instance with CPU and memory options similar to your application's current on-premises virtual machine. Install the Cloud Monitoring agent, and deploy the third-party 
                                    application. Run a load test with normal traffic levels on the application, and follow the Rightsizing Recommendations in the Cloud Console.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>156. Your company has a Google Cloud project that uses BigQuery for data warehousing. They have a VPN tunnel between the on-premises environment and Google
                                    Cloud that is configured with Cloud VPN. The security team wants to avoid data exfiltration by malicious insiders, compromised code, and accidental oversharing.
                                    What should they do?</p>
                                 <p>A. Configure Private Google Access for on-premises only.<br/> 
                                    B. Perform the following tasks: 1. Create a service account. 2. Give the BigQuery JobUser role and Storage Reader role to the service account. 3. Remove all other IAM access from the project.<br/> 
                                    C. Configure VPC Service Controls and configure Private Google Access.<br/>
                                    D. Configure Private Google Access.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>157. You are working at an institution that processes medical data. You are migrating several workloads onto Google Cloud. Company policies require all workloads to run on physically separated hardware, and workloads from different clients must also be separated. You created a sole-tenant node group and added a node for each client. You need to deploy the workloads on these dedicated hosts. What should you do?
                                    </p>
                                 <p>A. Add the node group name as a network tag when creating Compute Engine instances in order to host each workload on the correct node group.<br/> 
                                    B. Add the node name as a network tag when creating Compute Engine instances in order to host each workload on the correct node.<br/> 
                                    C. Use node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group.<br/>
                                    D. Use node affinity labels based on the node name when creating Compute Engine instances in order to host each workload on the correct node.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>158. Your companyג€™s test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full test suite takes several hours to complete, running on a limited number of on-premises servers reserved for testing. Your company wants to move the testing infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible.
                                    Which cloud infrastructure should you recommend?</p>
                                 <p>A. Google Compute Engine unmanaged instance groups and Network Load Balancer<br/> 
                                    B. Google Compute Engine managed instance groups with auto-scaling<br/> 
                                    C. Google Cloud Dataproc to run Apache Hadoop jobs to process each test<br/>
                                    D. Google App Engine with Google StackDriver for logging</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: B</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>159. A lead software engineer tells you that his new application design uses websockets and HTTP sessions that are not distributed across the web servers. You want to help him ensure his application will run properly on Google Cloud Platform.
                                    What should you do?
                                    </p>
                                 <p>A. Help the engineer to convert his websocket code to use HTTP streaming<br/> 
                                    B. Review the encryption requirements for websocket connections with the security team<br/> 
                                    C. Meet with the cloud operations team and the engineer to discuss load balancer options<br/>
                                    D. Help the engineer redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>160. The application reliability team at your company this added a debug feature to their backend service to send all server events to Google Cloud Storage for eventual analysis. The event records are at least 50 KB and at most 15 MB and are expected to peak at 3,000 events per second. You want to minimize data loss.
                                    Which process should you implement?</p>
                                 <p>A. Append metadata to file body ג€¢ Compress individual files ג€¢ Name files with serverName ג€" Timestamp ג€¢ Create a new bucket if bucket is older than 1 hour and save individual files to the new bucket. Otherwise, save files to existing bucket.<br/> 
                                    B. Batch every 10,000 events with a single manifest file for metadata ג€¢ Compress event files and manifest file into a single archive file ג€¢ Name files using serverName ג€" EventSequence ג€¢ Create a new bucket if bucket is older than 1 day and save the single archive file to the new bucket. Otherwise, save the single archive file to existing bucket.<br/> 
                                    C. Compress individual files ג€¢ Name files with serverName ג€" EventSequence ג€¢ Save files to one bucket ג€¢ Set custom metadata headers for each object after saving<br/>
                                    D. Append metadata to file body ג€¢ Compress individual files ג€¢ Name files with a random prefix pattern ג€¢ Save files to one bucket</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>161. A recent audit revealed that a new network was created in your GCP project. In this network, a GCE instance has an SSH port open to the world. You want to discover this networkג€™s origin.
                                    What should you do?</p>
                                 <p>A. Search for Create VM entry in the Stackdriver alerting console<br/> 
                                    B. Navigate to the Activity page in the Home section. Set category to Data Access and search for Create VM entry<br/> 
                                    C. In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry<br/>
                                    D. Connect to the GCE instance using project SSH keys. Identify previous logins in system logs, and match these with the project owners list</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>162. You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage and replace the copy easily if there are changes on the production virtual machine. You will deploy the copy as a new instance in a different project in the US-East region.
                                    What steps must you take?</p>
                                 <p>A. Use the Linux dd and netcat commands to copy and stream the root disk contents to a new virtual machine instance in the US-East region.<br/> 
                                    B. Create a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual machine instance in the US-East region.<br/> 
                                    C. Create an image file from the root disk with Linux dd command, create a new virtual machine instance in the US-East region<br/>
                                    D. Create a snapshot of the root disk, create an image file in Google Cloud Storage from the snapshot, and create a new virtual machine instance in the US-East region using the image file the root disk.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>163. Your company runs several databases on a single MySQL instance. They need to take backups of a specific database at regular intervals. The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.
                                    How should you configure the storage?</p>
                                 <p>A. Configure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots.<br/> 
                                    B. Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to Google Cloud Storage.<br/> 
                                    C. Use gcsfise to mount a Google Cloud Storage bucket as a volume directly on the instance and write backups to the mounted location using mysqldump.<br/>
                                    D. Mount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array and use LVM to create snapshots to send to Cloud Storage</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: B</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>164. You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud services that run on Google Compute Engine with Cloud
                                    Bigtable.
                                    Which three requirements should they include? (Choose three.)</p>
                                 <p>A. Ensure that the load tests validate the performance of Cloud Bigtable<br/> 
                                    B. Create a separate Google Cloud project to use for the load-testing environment<br/> 
                                    C. Schedule the load-testing tool to regularly run against the production environment<br/>
                                    D. Ensure all third-party systems your services use is capable of handling high load<br/>
                                    E. Instrument the production services to record every transaction for replay by the load-testing tool<br/>
                                    F. Instrument the load-testing tool and the target services with detailed logging and metrics collection</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: AEF</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>165. Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin.
                                    What Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team?</p>
                                 <p>A. Org viewer, project owner<br/> 
                                    B. Org viewer, project viewer<br/> 
                                    C. Org admin, project browser<br/>
                                    D. Project owner, network admin</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: B</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>166. Your company places a high value on being responsive and meeting customer needs quickly. Their primary business objectives are release speed and agility. You want to reduce the chance of security errors being accidentally introduced.
                                    Which two actions can you take? (Choose two.)</p>
                                 <p>A. Ensure every code check-in is peer reviewed by a security SME<br/> 
                                    B. Use source code security analyzers as part of the CI/CD pipeline<br/> 
                                    C. Ensure you have stubs to unit test all interfaces between components<br/>
                                    D. Enable code signing and a trusted binary repository integrated with your CI/CD pipeline<br/>
                                    E. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: DE</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>167. You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes.
                                    What should you do?</p>
                                 <p>A. Add additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize CLUSTER_Name ג€" -size 10<br/> 
                                    B. Add a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE - -tags enable- autoscaling max-nodes-10<br/> 
                                    C. Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10<br/>
                                    D. Create a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10 and redeploy your application</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: B</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>168. Your marketing department wants to send out a promotional email campaign. The development team wants to minimize direct operation management. They project a wide range of possible customer responses, from 100 to 500,000 click-through per day. The link leads to a simple website that explains the promotion and collects user information and preferences.
                                    Which infrastructure should you recommend? (Choose two.)</p>
                                 <p>A. Use Google App Engine to serve the website and Google Cloud Datastore to store user data.<br/> 
                                    B. Use a Google Container Engine cluster to serve the website and store data to persistent disk.<br/> 
                                    C. Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.<br/>
                                    D. Use a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: AC</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>169. Your company just finished a rapid lift and shift to Google Compute Engine for your compute needs. You have another 9 months to design and deploy a more cloud-native solution. Specifically, you want a system that is no-ops and auto-scaling.
                                    Which two compute products should you choose? (Choose two.)</p>
                                 <p>A. Compute Engine with containers<br/> 
                                    B. Google Kubernetes Engine with containers<br/> 
                                    C. Google App Engine Standard Environment<br/>
                                    D. Compute Engine with custom instance types<br/>
                                    E. Compute Engine with managed instance groups</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: BC</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>170. One of your primary business objectives is being able to trust the data stored in your application. You want to log all changes to the application data.
                                    How can you design your logging system to verify authenticity of your logs?</p>
                                 <p>A. Write the log concurrently in the cloud and on premises<br/> 
                                    B. Use a SQL database and limit who can modify the log table<br/> 
                                    C. Digitally sign each timestamp and log entry and store the signature<br/>
                                    D. Create a JSON dump of each log entry and store it in Google Cloud Storage</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>171.Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created Google Cloud projects outside of the Google Cloud Organization.
                                    You want to create an Organization structure that allows developers to create projects, but prevents them from modifying production projects. You want to manage policies for all projects centrally and be able to set more restrictive policies for production projects.
                                    You want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-recommended practices. Now should you design the Organization structure?</p>
                                 <p>A. 1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both Organizations. 5. Additionally, set the production policies on the original Organization.<br/> 
                                    B. 1. Create a folder under the Organization resource named ג€Production.ג€ 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the ג€Productionג€ folder.<br/> 
                                    C. 1. Create folders under the Organization resource named ג€Developmentג€ and ג€Production.ג€ 2. Grant all developers the Project Creator IAM role on the ג€Developmentג€ folder. 3. Move the developer projects into the ג€Developmentג€ folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the ג€Productionג€ folder.<br/>
                                    D. 1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator IAM role on the Organization. 3. Create development projects outside of the Organization using the developer Google Workspace accounts. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the individual production projects.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>172. Your company has an application running on Compute Engine that allows users to play their favorite music. There are a fixed number of instances. Files are stored in Cloud Storage, and data is streamed directly to users. Users are reporting that they sometimes need to attempt 
                                    to play popular songs multiple times before they are successful. You need to improve the performance of the application. What should you do?</p>
                                 <p>A. 1. Mount the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances. 2. Serve music files directly from the backend Compute Engine instance.<br/> 
                                    B. 1. Create a Cloud Filestore NFS volume and attach it to the backend Compute Engine instances. 2. Download popular songs in Cloud Filestore. 3. Serve music files directly from the backend Compute Engine instance.<br/> 
                                    C. 1. Copy popular songs into CloudSQL as a blob. 2. Update application code to retrieve data from CloudSQL when Cloud Storage is overloaded.<br/>
                                    D. 1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and configure it with two backends: ג—‹ Managed instance group ג—‹ Cloud Storage bucket 3. Enable Cloud CDN on the bucket backend.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>173. The operations team in your company wants to save Cloud VPN log events for one year. You need to configure the cloud infrastructure to save the logs. What should you do?</p>
                                 <p>A. Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.<br/> 
                                    B. Enable the Compute Engine API, and then enable logging on the firewall rules that match the traffic you want to save.<br/> 
                                    C. Set up a Cloud Logging Dashboard titled Cloud VPN Logs, and then add a chart that queries for the VPN metrics over a one-year time period.<br/>
                                    D. Set up a filter in Cloud Logging and a topic in Pub/Sub to publish the logs.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>174. You are working with a data warehousing team that performs data analysis. The team needs to process data from external
                                     partners, but the data contains personally identifiable information (PII). You need to process and store the data without storing any of the PIIE data. What should you do?</p>
                                 <p>A. Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.<br/> 
                                    B. Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, store all non-PII data in BigQuery and store all PII data in a Cloud Storage bucket that has a retention policy set.<br/> 
                                    C. Ask the external partners to upload all data on Cloud Storage. Configure Bucket Lock for the bucket. Create a Dataflow pipeline to read the data from the bucket. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.<br/>
                                    D. Ask the external partners to import all data in your BigQuery dataset. Create a dataflow pipeline to copy the data into a new table. As part of the Dataflow bucket, skip all data in columns that have PII data</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>175. You want to allow your operations team to store logs from all the production projects in your Organization, without including logs from other projects. 
                                    All of the production projects are contained in a folder. You want to ensure that all logs for existing and new production projects are captured automatically. What should you do?</p>
                                 <p>A. Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.<br/> 
                                    B. Create an aggregated export on the Organization resource. Set the log sink to be a Cloud Storage bucket in an operations project.<br/> 
                                    C. Create log exports in the production projects. Set the log sinks to be a Cloud Storage bucket in an operations project.<br/>
                                    D. Create log exports in the production projects. Set the log sinks to be BigQuery datasets in the production projects, and grant IAM access to the operations team to run queries on the datasets.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>176. Your company has an application that is running on multiple instances of Compute Engine. It generates 1 TB per day of logs. For compliance reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30 days. 
                                    After that, they just need to be retained for audit purposes. You want to implement a storage solution that is compliant, minimizes costs, and follows Google-recommended practices. What should you do?</p>
                                 <p>A. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.<br/> 
                                    B. 1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.<br/> 
                                    C. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set a time_partitioning_expiration of 30 days.<br/>
                                    D. 1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a time_partitioning_expiration of 30 days.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>177. Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been configured as well. 
                                    The security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users outside the domain from gaining permissions from now on. What should they do?</p>
                                 <p>A. Configure an organization policy to restrict identities by domain.<br/> 
                                    B. Configure an organization policy to block creation of service accounts.<br/> 
                                    C. Configure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that donג€™t belong to the Cloud Identity domain from all projects.<br/>
                                    D. Create a technical user (e.g., crawler@yourdomain.com), and give it the project owner role at root organization level. Write a bash script that: ג€¢ Lists all the IAM rules of all projects within the organization. ג€¢ Deletes all users that do not belong to the company domain. Create a Compute Engine instance in a project within the 
                                    Organization and configure gcloud to be executed with technical user credentials. Configure a cron job that executes the bash script every hour.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A/div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>178. Your company has an application running on Google Cloud that is collecting data from thousands of physical devices that are globally distributed. Data is published to Pub/Sub and streamed in real time into an SSD Cloud Bigtable cluster via a Dataflow pipeline. The operations team informs you that your Cloud
                                    Bigtable cluster has a hotspot, and queries are taking longer than expected. You need to resolve the problem and prevent it from happening in the future. What should you do?
                                    </p>
                                 <p>A. Advise your clients to use HBase APIs instead of NodeJS APIs.<br/> 
                                    B. Delete records older than 30 days.<br/> 
                                    C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.<br/>
                                    D. Double the number of nodes you currently have.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>179. Your company has a Google Cloud project that uses BigQuery for data warehousing. There are some tables that contain personally identifiable information (PII).
                                    Only the compliance team may access the PII. The other information in the tables must be available to the data science team. You want to minimize cost and the time it takes to assign appropriate access to the tables. What should you do?</p>
                                 <p>A. 1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.
                                    <br/> 
                                    B. 1. From the dataset where you have the source data, create materialized views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.<br/> 
                                    C. 1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset.<br/>
                                    D. 1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>180. Your operations team currently stores 10 TB of data in an object storage service from a third-party provider. They want to move this data to a Cloud Storage bucket as quickly as possible, following Google-recommended practices. They want to minimize the cost of this data migration. Which approach should they use?</p>
                                 <p>A. Use the gsutil mv command to move the data.<br/> 
                                    B. Use the Storage Transfer Service to move the data.<br/> 
                                    C. Download the data to a Transfer Appliance, and ship it to Google.<br/>
                                    D. Download the data to the on-premises data center, and upload it to the Cloud Storage bucket.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: B</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>181. Introductory Info
                                    Company Overview -
                                    JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One of the companyג€™s core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon output by 50% over the next 5 years.
                                    
                                    Company Background -
                                    JencoMart started as a general store in 1931, and has grown into one of the worldג€™s leading brands, known for great value and customer service. Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart has little presence in Asia, but considers that market key for future growth.
                                    
                                    Solution Concept -
                                    JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no longer supported.
                                    
                                    Existing Technical Environment -
                                    JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.
                                    JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.
                                    Application: Customer loyalty portal
                                    LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.
                                    
                                    Database -
                                    Oracle Database stores user profiles
                                    - 20 TB
                                    - Complex table structure
                                    - Well maintained, clean data
                                    - Strong backup strategy
                                    PostgreSQL database stores user credentials
                                    - Single-homed in US West
                                    - No redundancy
                                    - Backed up every 12 hours
                                    - 100% uptime service level agreement (SLA)
                                    - Authenticates all users
                                    
                                    Compute -
                                    30 machines in US West Coast, each machine has:
                                    - Twin, dual core CPUs
                                    - 32 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    20 machines in US East Coast, each machine has:
                                    - Single, dual-core CPU
                                    - 24 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    
                                    Storage -
                                    Access to shared 100 TB SAN in each location
                                    Tape backup every week
                                    
                                    Business Requirements -
                                    Optimize for capacity during peak periods and value during off-peak periods
                                    Guarantee service availability and support
                                    Reduce on-premises footprint and associated financial and environmental impact
                                    Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase
                                    Expand services into Asia
                                    
                                    Technical Requirements -
                                    Assess key application for cloud suitability
                                    Modify applications for the cloud
                                    Move applications to a new infrastructure
                                    Leverage managed services wherever feasible
                                    Sunset 20% of capacity in existing data centers
                                    Decrease latency in Asia
                                    
                                    CEO Statement -
                                    JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the environment through ג€greenג€ initiatives and policies.
                                    
                                    CTO Statement -
                                    The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.
                                    
                                    CFO Statement -
                                    Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak periods and reduce costs.
                                    Question
                                    The JencoMart security team requires that all Google Cloud Platform infrastructure is deployed using a least privilege model with separation of duties for administration between production and development resources.
                                    What Google domain and project structure should you recommend?</p>
                                 <p>A. Create two G Suite accounts to manage users: one for development/test/staging and one for production. Each account should contain one project for every application<br/> 
                                    B. Create two G Suite accounts to manage users: one with a single project for all development applications and one with a single project for all production applications<br/> 
                                    C. Create a single G Suite account to manage users with each stage of each application in its own project<br/>
                                    D. Create a single G Suite account to manage users with one project for the development/test/staging environment and one project for the production environment</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>
                               
                               <div class="entry-content">
											<p>182. Introductory Info
                                    Company Overview -
                                    JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One of the companyג€™s core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon output by 50% over the next 5 years.
                                    
                                    Company Background -
                                    JencoMart started as a general store in 1931, and has grown into one of the worldג€™s leading brands, known for great value and customer service. Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart has little presence in Asia, but considers that market key for future growth.
                                    
                                    Solution Concept -
                                    JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no longer supported.
                                    
                                    Existing Technical Environment -
                                    JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.
                                    JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.
                                    Application: Customer loyalty portal
                                    LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.
                                    
                                    Database -
                                    Oracle Database stores user profiles
                                    - 20 TB
                                    - Complex table structure
                                    - Well maintained, clean data
                                    - Strong backup strategy
                                    PostgreSQL database stores user credentials
                                    - Single-homed in US West
                                    - No redundancy
                                    - Backed up every 12 hours
                                    - 100% uptime service level agreement (SLA)
                                    - Authenticates all users
                                    
                                    Compute -
                                    30 machines in US West Coast, each machine has:
                                    - Twin, dual core CPUs
                                    - 32 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    20 machines in US East Coast, each machine has:
                                    - Single, dual-core CPU
                                    - 24 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    
                                    Storage -
                                    Access to shared 100 TB SAN in each location
                                    Tape backup every week
                                    
                                    Business Requirements -
                                    Optimize for capacity during peak periods and value during off-peak periods
                                    Guarantee service availability and support
                                    Reduce on-premises footprint and associated financial and environmental impact
                                    Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase
                                    Expand services into Asia
                                    
                                    Technical Requirements -
                                    Assess key application for cloud suitability
                                    Modify applications for the cloud
                                    Move applications to a new infrastructure
                                    Leverage managed services wherever feasible
                                    Sunset 20% of capacity in existing data centers
                                    Decrease latency in Asia
                                    
                                    CEO Statement -
                                    JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the environment through ג€greenג€ initiatives and policies.
                                    
                                    CTO Statement -
                                    The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.
                                    
                                    CFO Statement -
                                    Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak periods and reduce costs.
                                    Question
                                    A few days after JencoMart migrates the user credentials database to Google Cloud Platform and shuts down the old server, the new database server stops responding to SSH connections. It is still serving database requests to the application servers correctly.
                                    What three steps should you take to diagnose the problem? (Choose three.)</p>
                                 <p>A. Delete the virtual machine (VM) and disks and create a new one<br/> 
                                    B. Delete the instance, attach the disk to a new VM, and investigate<br/> 
                                    C. Take a snapshot of the disk and connect to a new machine to investigate<br/>
                                    D. Check inbound firewall rules for the network the machine is connected to<br/>
                                    E. Connect the machine to another network with very simple firewall rules and investigate<br/>
                                    F. Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: CDF</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>183. Introductory Info
                                    Company Overview -
                                    JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One of the companyג€™s core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon output by 50% over the next 5 years.
                                    
                                    Company Background -
                                    JencoMart started as a general store in 1931, and has grown into one of the worldג€™s leading brands, known for great value and customer service. Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart has little presence in Asia, but considers that market key for future growth.
                                    
                                    Solution Concept -
                                    JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no longer supported.
                                    
                                    Existing Technical Environment -
                                    JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.
                                    JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.
                                    Application: Customer loyalty portal
                                    LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.
                                    
                                    Database -
                                    Oracle Database stores user profiles
                                    - 20 TB
                                    - Complex table structure
                                    - Well maintained, clean data
                                    - Strong backup strategy
                                    PostgreSQL database stores user credentials
                                    - Single-homed in US West
                                    - No redundancy
                                    - Backed up every 12 hours
                                    - 100% uptime service level agreement (SLA)
                                    - Authenticates all users
                                    
                                    Compute -
                                    30 machines in US West Coast, each machine has:
                                    - Twin, dual core CPUs
                                    - 32 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    20 machines in US East Coast, each machine has:
                                    - Single, dual-core CPU
                                    - 24 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    
                                    Storage -
                                    Access to shared 100 TB SAN in each location
                                    Tape backup every week
                                    
                                    Business Requirements -
                                    Optimize for capacity during peak periods and value during off-peak periods
                                    Guarantee service availability and support
                                    Reduce on-premises footprint and associated financial and environmental impact
                                    Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase
                                    Expand services into Asia
                                    
                                    Technical Requirements -
                                    Assess key application for cloud suitability
                                    Modify applications for the cloud
                                    Move applications to a new infrastructure
                                    Leverage managed services wherever feasible
                                    Sunset 20% of capacity in existing data centers
                                    Decrease latency in Asia
                                    
                                    CEO Statement -
                                    JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the environment through ג€greenג€ initiatives and policies.
                                    
                                    CTO Statement -
                                    The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.
                                    
                                    CFO Statement -
                                    Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak periods and reduce costs.
                                    Question
                                    JencoMart has decided to migrate user profile storage to Google Cloud Datastore and the application servers to Google Compute Engine (GCE). During the migration, the existing infrastructure will need access to Datastore to upload the data.
                                    What service account key-management strategy should you recommend?</p>
                                 <p>A. Provision service account keys for the on-premises infrastructure and for the GCE virtual machines (VMs)<br/> 
                                    B. Authenticate the on-premises infrastructure with a user account and provision service account keys for the VMs<br/> 
                                    C. Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the VMs<br/>
                                    D. Deploy a custom authentication service on GCE/Google Kubernetes Engine (GKE) for the on-premises infrastructure and use GCP managed keys for the VMs</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>184. Introductory Info
                                    Company Overview -
                                    JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One of the companyג€™s core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon output by 50% over the next 5 years.
                                    
                                    Company Background -
                                    JencoMart started as a general store in 1931, and has grown into one of the worldג€™s leading brands, known for great value and customer service. Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart has little presence in Asia, but considers that market key for future growth.
                                    
                                    Solution Concept -
                                    JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no longer supported.
                                    
                                    Existing Technical Environment -
                                    JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.
                                    JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.
                                    Application: Customer loyalty portal
                                    LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.
                                    
                                    Database -
                                    Oracle Database stores user profiles
                                    - 20 TB
                                    - Complex table structure
                                    - Well maintained, clean data
                                    - Strong backup strategy
                                    PostgreSQL database stores user credentials
                                    - Single-homed in US West
                                    - No redundancy
                                    - Backed up every 12 hours
                                    - 100% uptime service level agreement (SLA)
                                    - Authenticates all users
                                    
                                    Compute -
                                    30 machines in US West Coast, each machine has:
                                    - Twin, dual core CPUs
                                    - 32 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    20 machines in US East Coast, each machine has:
                                    - Single, dual-core CPU
                                    - 24 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    
                                    Storage -
                                    Access to shared 100 TB SAN in each location
                                    Tape backup every week
                                    
                                    Business Requirements -
                                    Optimize for capacity during peak periods and value during off-peak periods
                                    Guarantee service availability and support
                                    Reduce on-premises footprint and associated financial and environmental impact
                                    Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase
                                    Expand services into Asia
                                    
                                    Technical Requirements -
                                    Assess key application for cloud suitability
                                    Modify applications for the cloud
                                    Move applications to a new infrastructure
                                    Leverage managed services wherever feasible
                                    Sunset 20% of capacity in existing data centers
                                    Decrease latency in Asia
                                    
                                    CEO Statement -
                                    JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the environment through ג€greenג€ initiatives and policies.
                                    
                                    CTO Statement -
                                    The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.
                                    
                                    CFO Statement -
                                    Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak periods and reduce costs.
                                    Question
                                    JencoMart has built a version of their application on Google Cloud Platform that serves traffic to Asia. You want to measure success against their business and technical goals.
                                    Which metrics should you track?</p>
                                 <p>A. Error rates for requests from Asia<br/> 
                                    B. Latency difference between US and Asia<br/> 
                                    C. Total visits, error rates, and latency from Asia<br/>
                                    D. Total visits and average latency for users from Asia<br/>
                                    E. The number of character sets present in the database</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>185. Introductory Info
                                    Company Overview -
                                    JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One of the companyג€™s core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon output by 50% over the next 5 years.
                                    
                                    Company Background -
                                    JencoMart started as a general store in 1931, and has grown into one of the worldג€™s leading brands, known for great value and customer service. Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart has little presence in Asia, but considers that market key for future growth.
                                    
                                    Solution Concept -
                                    JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no longer supported.
                                    
                                    Existing Technical Environment -
                                    JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.
                                    JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.
                                    Application: Customer loyalty portal
                                    LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.
                                    
                                    Database -
                                    Oracle Database stores user profiles
                                    - 20 TB
                                    - Complex table structure
                                    - Well maintained, clean data
                                    - Strong backup strategy
                                    PostgreSQL database stores user credentials
                                    - Single-homed in US West
                                    - No redundancy
                                    - Backed up every 12 hours
                                    - 100% uptime service level agreement (SLA)
                                    - Authenticates all users
                                    
                                    Compute -
                                    30 machines in US West Coast, each machine has:
                                    - Twin, dual core CPUs
                                    - 32 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    20 machines in US East Coast, each machine has:
                                    - Single, dual-core CPU
                                    - 24 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    
                                    Storage -
                                    Access to shared 100 TB SAN in each location
                                    Tape backup every week
                                    
                                    Business Requirements -
                                    Optimize for capacity during peak periods and value during off-peak periods
                                    Guarantee service availability and support
                                    Reduce on-premises footprint and associated financial and environmental impact
                                    Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase
                                    Expand services into Asia
                                    
                                    Technical Requirements -
                                    Assess key application for cloud suitability
                                    Modify applications for the cloud
                                    Move applications to a new infrastructure
                                    Leverage managed services wherever feasible
                                    Sunset 20% of capacity in existing data centers
                                    Decrease latency in Asia
                                    
                                    CEO Statement -
                                    JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the environment through ג€greenג€ initiatives and policies.
                                    
                                    CTO Statement -
                                    The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.
                                    
                                    CFO Statement -
                                    Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak periods and reduce costs.
                                    Question
                                 <img src="https://www.examtopics.com/assets/media/exam-media/04136/0000700001.png">
                                 The migration of JencoMartג€™s application to Google Cloud Platform (GCP) is progressing too slowly. The infrastructure is shown in the diagram. You want to maximize throughput.
What are three potential bottlenecks? (Choose three.)
                                 </p>
                                 <p>A. A single VPN tunnel, which limits throughput<br/> 
                                    B. A tier of Google Cloud Storage that is not suited for this task<br/> 
                                    C. A copy command that is not suited to operate over long distances<br/>
                                    D. Fewer virtual machines (VMs) in GCP than on-premises machines<br/>
                                    E. A separate storage layer outside the VMs, which is not suited for this task <br/>
                                    F. Complicated internet connectivity between the on-premises infrastructure and GCP</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: ACE</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>186. Introductory Info
                                    Company Overview -
                                    JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One of the companyג€™s core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon output by 50% over the next 5 years.
                                    
                                    Company Background -
                                    JencoMart started as a general store in 1931, and has grown into one of the worldג€™s leading brands, known for great value and customer service. Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart has little presence in Asia, but considers that market key for future growth.
                                    
                                    Solution Concept -
                                    JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no longer supported.
                                    
                                    Existing Technical Environment -
                                    JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.
                                    JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.
                                    Application: Customer loyalty portal
                                    LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.
                                    
                                    Database -
                                    Oracle Database stores user profiles
                                    - 20 TB
                                    - Complex table structure
                                    - Well maintained, clean data
                                    - Strong backup strategy
                                    PostgreSQL database stores user credentials
                                    - Single-homed in US West
                                    - No redundancy
                                    - Backed up every 12 hours
                                    - 100% uptime service level agreement (SLA)
                                    - Authenticates all users
                                    
                                    Compute -
                                    30 machines in US West Coast, each machine has:
                                    - Twin, dual core CPUs
                                    - 32 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    20 machines in US East Coast, each machine has:
                                    - Single, dual-core CPU
                                    - 24 GB of RAM
                                    - Twin 250 GB HDD (RAID 1)
                                    
                                    Storage -
                                    Access to shared 100 TB SAN in each location
                                    Tape backup every week
                                    
                                    Business Requirements -
                                    Optimize for capacity during peak periods and value during off-peak periods
                                    Guarantee service availability and support
                                    Reduce on-premises footprint and associated financial and environmental impact
                                    Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase
                                    Expand services into Asia
                                    
                                    Technical Requirements -
                                    Assess key application for cloud suitability
                                    Modify applications for the cloud
                                    Move applications to a new infrastructure
                                    Leverage managed services wherever feasible
                                    Sunset 20% of capacity in existing data centers
                                    Decrease latency in Asia
                                    
                                    CEO Statement -
                                    JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the environment through ג€greenג€ initiatives and policies.
                                    
                                    CTO Statement -
                                    The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.
                                    
                                    CFO Statement -
                                    Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak periods and reduce costs.
                                    Question
                                    JencoMart wants to move their User Profiles database to Google Cloud Platform.
                                    Which Google Database should they use?</p>
                                 <p>A. Cloud Spanner<br/> 
                                    B. Google BigQuery<br/> 
                                    C. Google Cloud SQL<br/>
                                    D. Google Cloud Datastore</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>187. Introductory Info
                                    Company overview -
                                    Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.
                                    
                                    Solution concept -
                                    HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.
                                    
                                    Existing technical environment -
                                    HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:
                                    Existing content is stored in an object storage service on their existing public cloud provider.
                                    Video encoding and transcoding is performed on VMs created for each job.
                                    Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.
                                    
                                    Business requirements -
                                    HRLג€™s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:
                                    Support ability to expose the predictive models to partners.
                                    Increase predictive capabilities during and before races:
                                    ג—‹ Race results
                                    ג—‹ Mechanical failures
                                    ג—‹ Crowd sentiment
                                    Increase telemetry and create additional insights.
                                    Measure fan engagement with new predictions.
                                    Enhance global availability and quality of the broadcasts.
                                    Increase the number of concurrent viewers.
                                    Minimize operational complexity.
                                    Ensure compliance with regulations.
                                    Create a merchandising revenue stream.
                                    
                                    Technical requirements -
                                    Maintain or increase prediction throughput and accuracy.
                                    Reduce viewer latency.
                                    Increase transcoding performance.
                                    Create real-time analytics of viewer consumption patterns and engagement.
                                    Create a data mart to enable processing of large volumes of race data.
                                    
                                    Executive statement -
                                    Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.
                                    Question
                                    For this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You need to implement a custom card tokenization service that meets the following requirements:
                                    ג€¢ It must provide low latency at minimal cost.
                                    ג€¢ It must be able to identify duplicate credit cards and must not store plaintext card numbers.
                                    ג€¢ It should support annual key rotation.
                                    Which storage approach should you adopt for your tokenization service?</p>
                                 <p>A. Store the card data in Secret Manager after running a query to identify duplicates.<br/> 
                                    B. Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.<br/> 
                                    C. Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.<br/>
                                    D. Use column-level encryption to store the data in Cloud SQL.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: B</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>188. Introductory Info
                                    Company overview -
                                    Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.
                                    
                                    Solution concept -
                                    HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.
                                    
                                    Existing technical environment -
                                    HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:
                                    Existing content is stored in an object storage service on their existing public cloud provider.
                                    Video encoding and transcoding is performed on VMs created for each job.
                                    Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.
                                    
                                    Business requirements -
                                    HRLג€™s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:
                                    Support ability to expose the predictive models to partners.
                                    Increase predictive capabilities during and before races:
                                    ג—‹ Race results
                                    ג—‹ Mechanical failures
                                    ג—‹ Crowd sentiment
                                    Increase telemetry and create additional insights.
                                    Measure fan engagement with new predictions.
                                    Enhance global availability and quality of the broadcasts.
                                    Increase the number of concurrent viewers.
                                    Minimize operational complexity.
                                    Ensure compliance with regulations.
                                    Create a merchandising revenue stream.
                                    
                                    Technical requirements -
                                    Maintain or increase prediction throughput and accuracy.
                                    Reduce viewer latency.
                                    Increase transcoding performance.
                                    Create real-time analytics of viewer consumption patterns and engagement.
                                    Create a data mart to enable processing of large volumes of race data.
                                    
                                    Executive statement -
                                    Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.
                                    Question
                                    For this question, refer to the Helicopter Racing League (HRL) case study. Recently HRL started a new regional racing league in Cape Town, South Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the Content Delivery Network provider, Fastly. HRL needs to allow traffic coming from all of the Fastly IP address ranges into their Virtual Private Cloud network (VPC network). 
                                    You are a member of the HRL security team and you need to configure the update that will allow only the Fastly IP address ranges through the External HTTP(S) load balancer. Which command should you use?</p>
                                 <p>A. <img sr="https://www.examtopics.com/assets/media/exam-media/04136/0006100001.png"><br/> 
                                    B. <img src="https://www.examtopics.com/assets/media/exam-media/04136/0006100002.png"><br/> 
                                    C. <img src="https://www.examtopics.com/assets/media/exam-media/04136/0006100003.png"><br/>
                                    D. <img src="https://www.examtopics.com/assets/media/exam-media/04136/0006100004.png"></p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>189. Introductory Info
                                    Company overview -
                                    Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.
                                    
                                    Solution concept -
                                    HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.
                                    
                                    Existing technical environment -
                                    HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:
                                    Existing content is stored in an object storage service on their existing public cloud provider.
                                    Video encoding and transcoding is performed on VMs created for each job.
                                    Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.
                                    
                                    Business requirements -
                                    HRLג€™s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:
                                    Support ability to expose the predictive models to partners.
                                    Increase predictive capabilities during and before races:
                                    ג—‹ Race results
                                    ג—‹ Mechanical failures
                                    ג—‹ Crowd sentiment
                                    Increase telemetry and create additional insights.
                                    Measure fan engagement with new predictions.
                                    Enhance global availability and quality of the broadcasts.
                                    Increase the number of concurrent viewers.
                                    Minimize operational complexity.
                                    Ensure compliance with regulations.
                                    Create a merchandising revenue stream.
                                    
                                    Technical requirements -
                                    Maintain or increase prediction throughput and accuracy.
                                    Reduce viewer latency.
                                    Increase transcoding performance.
                                    Create real-time analytics of viewer consumption patterns and engagement.
                                    Create a data mart to enable processing of large volumes of race data.
                                    
                                    Executive statement -
                                    Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.
                                    Question
                                    For this question, refer to the Helicopter Racing League (HRL) case study. The HRL development team releases a new version of their predictive capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has developed an in-house penetration test Cloud Function called
                                    Airwolf. The security team wants to run Airwolf against the predictive capability application as soon as it is released every Tuesday. You need to set up Airwolf to run at the recurring weekly cadence. What should you do?</p>
                                 <p>A. Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.<br/> 
                                    B. Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.<br/> 
                                    C. Configure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function.<br/> 
                                    D. Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>190. Introductory Info
                                    Company overview -
                                    Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.
                                    
                                    Solution concept -
                                    HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.
                                    
                                    Existing technical environment -
                                    HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:
                                    Existing content is stored in an object storage service on their existing public cloud provider.
                                    Video encoding and transcoding is performed on VMs created for each job.
                                    Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.
                                    
                                    Business requirements -
                                    HRLג€™s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:
                                    Support ability to expose the predictive models to partners.
                                    Increase predictive capabilities during and before races:
                                    ג—‹ Race results
                                    ג—‹ Mechanical failures
                                    ג—‹ Crowd sentiment
                                    Increase telemetry and create additional insights.
                                    Measure fan engagement with new predictions.
                                    Enhance global availability and quality of the broadcasts.
                                    Increase the number of concurrent viewers.
                                    Minimize operational complexity.
                                    Ensure compliance with regulations.
                                    Create a merchandising revenue stream.
                                    
                                    Technical requirements -
                                    Maintain or increase prediction throughput and accuracy.
                                    Reduce viewer latency.
                                    Increase transcoding performance.
                                    Create real-time analytics of viewer consumption patterns and engagement.
                                    Create a data mart to enable processing of large volumes of race data.
                                    
                                    Executive statement -
                                    Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.
                                    Question
                                    For this question, refer to the Helicopter Racing League (HRL) case study. HRL wants better prediction accuracy from their ML prediction models. They want you to use Googleג€™s AI Platform so HRL can understand and interpret the predictions. What should you do?</p>
                                 <p>A. Use Explainable AI.<br/> 
                                    B. Use Vision AI.<br/> 
                                    C. Use Google Cloudג€™s operations suite.<br/>
                                    D. Use Jupyter Notebooks.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>191. Introductory Info
                                    Company overview -
                                    Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.
                                    
                                    Solution concept -
                                    HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.
                                    
                                    Existing technical environment -
                                    HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:
                                    Existing content is stored in an object storage service on their existing public cloud provider.
                                    Video encoding and transcoding is performed on VMs created for each job.
                                    Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.
                                    
                                    Business requirements -
                                    HRLג€™s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:
                                    Support ability to expose the predictive models to partners.
                                    Increase predictive capabilities during and before races:
                                    ג—‹ Race results
                                    ג—‹ Mechanical failures
                                    ג—‹ Crowd sentiment
                                    Increase telemetry and create additional insights.
                                    Measure fan engagement with new predictions.
                                    Enhance global availability and quality of the broadcasts.
                                    Increase the number of concurrent viewers.
                                    Minimize operational complexity.
                                    Ensure compliance with regulations.
                                    Create a merchandising revenue stream.
                                    
                                    Technical requirements -
                                    Maintain or increase prediction throughput and accuracy.
                                    Reduce viewer latency.
                                    Increase transcoding performance.
                                    Create real-time analytics of viewer consumption patterns and engagement.
                                    Create a data mart to enable processing of large volumes of race data.
                                    
                                    Executive statement -
                                    Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.
                                    Question
                                    For this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost-effective approach for storing their race data such as telemetry. They want to keep all historical records, train models using only the previous season's data, and plan for data growth in terms of volume and information collected. You need to propose a data solution. Considering HRL business requirements and the goals expressed by CEO S. Hawke, what should you do?</p>
                                 <p>A. Use Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season and event.<br/> 
                                    B. Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a primary key.<br/> 
                                    C. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.<br/>
                                    D. Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database instances for each season.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>192. Introductory Info
                                    Company overview -
                                    Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.
                                    
                                    Solution concept -
                                    HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.
                                    
                                    Existing technical environment -
                                    HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:
                                    Existing content is stored in an object storage service on their existing public cloud provider.
                                    Video encoding and transcoding is performed on VMs created for each job.
                                    Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.
                                    
                                    Business requirements -
                                    HRLג€™s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:
                                    Support ability to expose the predictive models to partners.
                                    Increase predictive capabilities during and before races:
                                    ג—‹ Race results
                                    ג—‹ Mechanical failures
                                    ג—‹ Crowd sentiment
                                    Increase telemetry and create additional insights.
                                    Measure fan engagement with new predictions.
                                    Enhance global availability and quality of the broadcasts.
                                    Increase the number of concurrent viewers.
                                    Minimize operational complexity.
                                    Ensure compliance with regulations.
                                    Create a merchandising revenue stream.
                                    
                                    Technical requirements -
                                    Maintain or increase prediction throughput and accuracy.
                                    Reduce viewer latency.
                                    Increase transcoding performance.
                                    Create real-time analytics of viewer consumption patterns and engagement.
                                    Create a data mart to enable processing of large volumes of race data.
                                    
                                    Executive statement -
                                    Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.
                                    Question
                                    For this question, refer to the Helicopter Racing League (HRL) case study. A recent finance audit of cloud infrastructure noted an exceptionally high number of
                                    Compute Engine instances are allocated to do video encoding and transcoding. You suspect that these Virtual Machines are zombie machines that were not deleted after their workloads completed. You need to quickly get a list of which VM instances are idle. What should you do?</p>
                                 <p>A. Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.<br/> 
                                    B. Use the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.<br/> 
                                    C. Use the gcloud recommender command to list the idle virtual machine instances.<br/>
                                    D. From the Google Console, identify which Compute Engine instances in the managed instance groups are no longer responding to health check probes.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>193. Introductory Info
                                    Company overview -
                                    EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi- national medical offices, hospitals, and insurance providers.
                                    
                                    Solution concept -
                                    Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google
                                    Cloud has been chosen to replace their current colocation facilities.
                                    
                                    Existing technical environment -
                                    EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.
                                    Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).
                                    EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time.
                                    Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.
                                    
                                    Business requirements -
                                    ג€¢ On-board new insurance providers as quickly as possible.
                                    ג€¢ Provide a minimum 99.9% availability for all customer-facing systems.
                                    ג€¢ Provide centralized visibility and proactive action on system performance and usage.
                                    ג€¢ Increase ability to provide insights into healthcare trends.
                                    ג€¢ Reduce latency to all customers.
                                    ג€¢ Maintain regulatory compliance.
                                    ג€¢ Decrease infrastructure administration costs.
                                    ג€¢ Make predictions and generate reports on industry trends based on provider data.
                                    
                                    Technical requirements -
                                    ג€¢ Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.
                                    ג€¢ Provide a consistent way to manage customer-facing applications that are container-based.
                                    ג€¢ Provide a secure and high-performance connection between on-premises systems and Google Cloud.
                                    ג€¢ Provide consistent logging, log retention, monitoring, and alerting capabilities.
                                    ג€¢ Maintain and manage multiple container-based environments.
                                    ג€¢ Dynamically scale and provision new environments.
                                    ג€¢ Create interfaces to ingest and process data from new providers.
                                    
                                    Executive statement -
                                    Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.
                                    Question
                                    For this question, refer to the EHR Healthcare case study. You are responsible for ensuring that EHR's use of Google Cloud will pass an upcoming privacy compliance audit. What should you do? (Choose two.)</p>
                                 <p>A. Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page.<br/> 
                                    B. Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud.<br/> 
                                    C. Use Firebase Authentication for EHR's user facing applications.<br/>
                                    D. Implement Prometheus to detect and prevent security breaches on EHR's web-based applications.<br/>
                                    E. Use GKE private clusters for all Kubernetes workloads.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: BD</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>194. Introductory Info
                                    Company overview -
                                    EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi- national medical offices, hospitals, and insurance providers.
                                    
                                    Solution concept -
                                    Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google
                                    Cloud has been chosen to replace their current colocation facilities.
                                    
                                    Existing technical environment -
                                    EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.
                                    Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).
                                    EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time.
                                    Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.
                                    
                                    Business requirements -
                                    ג€¢ On-board new insurance providers as quickly as possible.
                                    ג€¢ Provide a minimum 99.9% availability for all customer-facing systems.
                                    ג€¢ Provide centralized visibility and proactive action on system performance and usage.
                                    ג€¢ Increase ability to provide insights into healthcare trends.
                                    ג€¢ Reduce latency to all customers.
                                    ג€¢ Maintain regulatory compliance.
                                    ג€¢ Decrease infrastructure administration costs.
                                    ג€¢ Make predictions and generate reports on industry trends based on provider data.
                                    
                                    Technical requirements -
                                    ג€¢ Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.
                                    ג€¢ Provide a consistent way to manage customer-facing applications that are container-based.
                                    ג€¢ Provide a secure and high-performance connection between on-premises systems and Google Cloud.
                                    ג€¢ Provide consistent logging, log retention, monitoring, and alerting capabilities.
                                    ג€¢ Maintain and manage multiple container-based environments.
                                    ג€¢ Dynamically scale and provision new environments.
                                    ג€¢ Create interfaces to ingest and process data from new providers.
                                    
                                    Executive statement -
                                    Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.
                                    Question
                                    For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for securely deploying workloads to Google Cloud. You also need to ensure that only verified containers are deployed using Google Cloud services. What should you do? (Choose two.)</p>
                                 <p>A. Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline.<br/> 
                                    B. Configure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline.<br/> 
                                    C. Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry.<br/>
                                    D. Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: AD</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>195. ntroductory Info
                                    Company overview -
                                    EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi- national medical offices, hospitals, and insurance providers.
                                    
                                    Solution concept -
                                    Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google
                                    Cloud has been chosen to replace their current colocation facilities.
                                    
                                    Existing technical environment -
                                    EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.
                                    Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).
                                    EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time.
                                    Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.
                                    
                                    Business requirements -
                                    ג€¢ On-board new insurance providers as quickly as possible.
                                    ג€¢ Provide a minimum 99.9% availability for all customer-facing systems.
                                    ג€¢ Provide centralized visibility and proactive action on system performance and usage.
                                    ג€¢ Increase ability to provide insights into healthcare trends.
                                    ג€¢ Reduce latency to all customers.
                                    ג€¢ Maintain regulatory compliance.
                                    ג€¢ Decrease infrastructure administration costs.
                                    ג€¢ Make predictions and generate reports on industry trends based on provider data.
                                    
                                    Technical requirements -
                                    ג€¢ Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.
                                    ג€¢ Provide a consistent way to manage customer-facing applications that are container-based.
                                    ג€¢ Provide a secure and high-performance connection between on-premises systems and Google Cloud.
                                    ג€¢ Provide consistent logging, log retention, monitoring, and alerting capabilities.
                                    ג€¢ Maintain and manage multiple container-based environments.
                                    ג€¢ Dynamically scale and provision new environments.
                                    ג€¢ Create interfaces to ingest and process data from new providers.
                                    
                                    Executive statement -
                                    Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.
                                    Question
                                    You need to upgrade the EHR connection to comply with their requirements. The new connection design must support business-critical needs and meet the same network and security policy requirements. What should you do?</p>
                                 <p>A. Add a new Dedicated Interconnect connection.<br/> 
                                    B. Upgrade the bandwidth on the Dedicated Interconnect connection to 100 G.<br/> 
                                    C. Add three new Cloud VPN connections.<br/>
                                    D. Add a new Carrier Peering connection.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>196. Introductory Info
                                    Company overview -
                                    EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi- national medical offices, hospitals, and insurance providers.
                                    
                                    Solution concept -
                                    Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google
                                    Cloud has been chosen to replace their current colocation facilities.
                                    
                                    Existing technical environment -
                                    EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.
                                    Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).
                                    EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time.
                                    Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.
                                    
                                    Business requirements -
                                    ג€¢ On-board new insurance providers as quickly as possible.
                                    ג€¢ Provide a minimum 99.9% availability for all customer-facing systems.
                                    ג€¢ Provide centralized visibility and proactive action on system performance and usage.
                                    ג€¢ Increase ability to provide insights into healthcare trends.
                                    ג€¢ Reduce latency to all customers.
                                    ג€¢ Maintain regulatory compliance.
                                    ג€¢ Decrease infrastructure administration costs.
                                    ג€¢ Make predictions and generate reports on industry trends based on provider data.
                                    
                                    Technical requirements -
                                    ג€¢ Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.
                                    ג€¢ Provide a consistent way to manage customer-facing applications that are container-based.
                                    ג€¢ Provide a secure and high-performance connection between on-premises systems and Google Cloud.
                                    ג€¢ Provide consistent logging, log retention, monitoring, and alerting capabilities.
                                    ג€¢ Maintain and manage multiple container-based environments.
                                    ג€¢ Dynamically scale and provision new environments.
                                    ג€¢ Create interfaces to ingest and process data from new providers.
                                    
                                    Executive statement -
                                    Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.
                                    Question
                                    For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for hybrid connectivity between EHR's on-premises systems and Google Cloud. You want to follow Google's recommended practices for production-level applications. Considering the EHR Healthcare business and technical requirements, what should you do?</p>
                                 <p>A. Configure two Partner Interconnect connections in one metro (City), and make sure the Interconnect connections are placed in different metro zones.<br/> 
                                    B. Configure two VPN connections from on-premises to Google Cloud, and make sure the VPN devices on-premises are in separate racks.<br/> 
                                    C. Configure Direct Peering between EHR Healthcare and Google Cloud, and make sure you are peering at least two Google locations.<br/>
                                    D. Configure two Dedicated Interconnect connections in one metro (City) and two connections in another metro, and make sure the Interconnect connections are placed in different metro zones.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: D</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>197. Introductory Info
                                    Company overview -
                                    EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi- national medical offices, hospitals, and insurance providers.
                                    
                                    Solution concept -
                                    Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google
                                    Cloud has been chosen to replace their current colocation facilities.
                                    
                                    Existing technical environment -
                                    EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.
                                    Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).
                                    EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time.
                                    Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.
                                    
                                    Business requirements -
                                    ג€¢ On-board new insurance providers as quickly as possible.
                                    ג€¢ Provide a minimum 99.9% availability for all customer-facing systems.
                                    ג€¢ Provide centralized visibility and proactive action on system performance and usage.
                                    ג€¢ Increase ability to provide insights into healthcare trends.
                                    ג€¢ Reduce latency to all customers.
                                    ג€¢ Maintain regulatory compliance.
                                    ג€¢ Decrease infrastructure administration costs.
                                    ג€¢ Make predictions and generate reports on industry trends based on provider data.
                                    
                                    Technical requirements -
                                    ג€¢ Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.
                                    ג€¢ Provide a consistent way to manage customer-facing applications that are container-based.
                                    ג€¢ Provide a secure and high-performance connection between on-premises systems and Google Cloud.
                                    ג€¢ Provide consistent logging, log retention, monitoring, and alerting capabilities.
                                    ג€¢ Maintain and manage multiple container-based environments.
                                    ג€¢ Dynamically scale and provision new environments.
                                    ג€¢ Create interfaces to ingest and process data from new providers.
                                    
                                    Executive statement -
                                    Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.
                                    Question
                                    For this question, refer to the EHR Healthcare case study. You are a developer on the EHR customer portal team. Your team recently migrated the customer portal application to Google Cloud. The load has increased on the application servers, and now the application is logging many timeout errors. You recently incorporated Pub/Sub into the application architecture, and the application is not logging any Pub/Sub publishing errors. You want to improve publishing latency.
                                    What should you do?</p>
                                 <p>A. Increase the Pub/Sub Total Timeout retry value.<br/> 
                                    B. Move from a Pub/Sub subscriber pull model to a push model.<br/> 
                                    C. Turn off Pub/Sub message batching.<br/>
                                    D. Create a backup Pub/Sub message queue.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: C</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>198. Introductory Info
                                    Company overview -
                                    EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi- national medical offices, hospitals, and insurance providers.
                                    
                                    Solution concept -
                                    Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google
                                    Cloud has been chosen to replace their current colocation facilities.
                                    
                                    Existing technical environment -
                                    EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.
                                    Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).
                                    EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time.
                                    Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.
                                    
                                    Business requirements -
                                    ג€¢ On-board new insurance providers as quickly as possible.
                                    ג€¢ Provide a minimum 99.9% availability for all customer-facing systems.
                                    ג€¢ Provide centralized visibility and proactive action on system performance and usage.
                                    ג€¢ Increase ability to provide insights into healthcare trends.
                                    ג€¢ Reduce latency to all customers.
                                    ג€¢ Maintain regulatory compliance.
                                    ג€¢ Decrease infrastructure administration costs.
                                    ג€¢ Make predictions and generate reports on industry trends based on provider data.
                                    
                                    Technical requirements -
                                    ג€¢ Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.
                                    ג€¢ Provide a consistent way to manage customer-facing applications that are container-based.
                                    ג€¢ Provide a secure and high-performance connection between on-premises systems and Google Cloud.
                                    ג€¢ Provide consistent logging, log retention, monitoring, and alerting capabilities.
                                    ג€¢ Maintain and manage multiple container-based environments.
                                    ג€¢ Dynamically scale and provision new environments.
                                    ג€¢ Create interfaces to ingest and process data from new providers.
                                    
                                    Executive statement -
                                    Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.
                                    Question
                                    For this question, refer to the EHR Healthcare case study. In the past, configuration errors put public IP addresses on backend servers that should not have been accessible from the Internet. You need to ensure that no one can put external IP addresses on backend Compute Engine instances and that external IP addresses can only be configured on frontend Compute Engine instances. What should you do?</p>
                                 <p>A. Create an Organizational Policy with a constraint to allow external IP addresses only on the frontend Compute Engine instances.<br/> 
                                    B. Revoke the compute.networkAdmin role from all users in the project with front end instances.<br/> 
                                    C. Create an Identity and Access Management (IAM) policy that maps the IT staff to the compute.networkAdmin role for the organization.<br/>
                                    D. Create a custom Identity and Access Management (IAM) role named GCE_FRONTEND with the compute.addresses.create permission.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>199. Introductory Info
                                    Company overview -
                                    EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi- national medical offices, hospitals, and insurance providers.
                                    
                                    Solution concept -
                                    Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google
                                    Cloud has been chosen to replace their current colocation facilities.
                                    
                                    Existing technical environment -
                                    EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.
                                    Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).
                                    EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time.
                                    Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.
                                    
                                    Business requirements -
                                    ג€¢ On-board new insurance providers as quickly as possible.
                                    ג€¢ Provide a minimum 99.9% availability for all customer-facing systems.
                                    ג€¢ Provide centralized visibility and proactive action on system performance and usage.
                                    ג€¢ Increase ability to provide insights into healthcare trends.
                                    ג€¢ Reduce latency to all customers.
                                    ג€¢ Maintain regulatory compliance.
                                    ג€¢ Decrease infrastructure administration costs.
                                    ג€¢ Make predictions and generate reports on industry trends based on provider data.
                                    
                                    Technical requirements -
                                    ג€¢ Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.
                                    ג€¢ Provide a consistent way to manage customer-facing applications that are container-based.
                                    ג€¢ Provide a secure and high-performance connection between on-premises systems and Google Cloud.
                                    ג€¢ Provide consistent logging, log retention, monitoring, and alerting capabilities.
                                    ג€¢ Maintain and manage multiple container-based environments.
                                    ג€¢ Dynamically scale and provision new environments.
                                    ג€¢ Create interfaces to ingest and process data from new providers.
                                    
                                    Executive statement -
                                    Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.
                                    Question
                                    For this question, refer to the EHR Healthcare case study. You are responsible for designing the Google Cloud network architecture for Google Kubernetes
                                    Engine. You want to follow Google best practices. Considering the EHR Healthcare business and technical requirements, what should you do to reduce the attack surface?</p>
                                 <p>A. Use a private cluster with a private endpoint with master authorized networks configured.<br/> 
                                    B. Use a public cluster with firewall rules and Virtual Private Cloud (VPC) routes.<br/> 
                                    C. Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called ג€cached_queriesג€.<br/>
                                    D. Set the memcache service level to shared. Create a key called ג€cached_queriesג€, and return database values from the key before using a query to Cloud SQL.</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>

                               <div class="entry-content">
											<p>200. Introductory Info
                                    Company Overview -
                                    Mountkirk Games makes online, session-based, multiplayer games for the most popular mobile platforms. They build all of their games using some server-side integration. Historically, they have used cloud providers to lease physical servers.
                                    Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers MySQL databases, and analytics tools.
                                    Their current model is to write game statistics to files and send them through an ETL tool that loads them into a centralized MySQL database for reporting.
                                    
                                    Solution Concept -
                                    Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the gameג€™s backend on Google Compute Engine so they can capture streaming metrics run intensive analytics, and take advantage of its autoscaling server environment and integrate with a managed NoSQL database.
                                    
                                    Business Requirements -
                                    Increase to a global footprint
                                    Improve uptime ג€" downtime is loss of players
                                    Increase efficiency of the cloud resources we use
                                    Reduce latency to all customers
                                    
                                    Technical Requirements -
                                    Requirements for Game Backend Platform
                                    1. Dynamically scale up or down based on game activity
                                    2. Connect to a managed NoSQL database service
                                    3. Run customize Linux distro
                                    Requirements for Game Analytics Platform
                                    1. Dynamically scale up or down based on game activity
                                    2. Process incoming data on the fly directly from the game servers
                                    3. Process data that arrives late because of slow mobile networks
                                    4. Allow SQL queries to access at least 10 TB of historical data
                                    5. Process files that are regularly uploaded by usersג€™ mobile devices
                                    6. Use only fully managed services
                                    
                                    CEO Statement -
                                    Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the gameג€™s reputation. Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide deeper insight into usage patterns so we can adapt the game to target users.
                                    
                                    CTO Statement -
                                    Our current technology stack cannot provide the scale we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from managing physical servers.
                                    
                                    CFO Statement -
                                    We are not capturing enough user demographic data, usage metrics, and other KPIs. As a result, we do not engage the right users, we are not confident that our marketing is targeting the right users, and we are not selling enough premium Blast-Ups inside the games, which dramatically impacts our revenue.
                                    Question
                                    Mountkirk Games wants you to design their new testing strategy. How should the test coverage differ from their existing backends on the other platforms?</p>
                                 <p>A. Tests should scale well beyond the prior approaches<br/> 
                                    B. Unit tests are no longer required, only end-to-end tests<br/> 
                                    C. Tests should be applied after the release is in the production environment<br/>
                                    D. Tests should include directly testing the Google Cloud Platform (GCP) infrastructure</p>
                                 <div class="su-spoiler su-spoiler-style-fancy su-spoiler-icon-chevron su-spoiler-closed" data-scroll-offset="0" data-anchor-in-url="no">
                                    <div class="su-spoiler-title" tabindex="0" role="button"><span class="su-spoiler-icon"></span>Reveal</div>
                                    <div class="su-spoiler-content su-u-clearfix su-u-trim"> Answer: A</div>
                                 </div>
                               </div>
                               
                              </div>
                           </div>
                           <div class="post-gap"></div>

                        </article>
                        <hr class="tall"/>
                     </div>
                  </div>
               </div>
            </div>
         </div>
         <div class="footer-wrapper ">
            <div id="footer" class="footer-3 show-ribbon">
               <div class="container">
                  <div class="footer-ribbon"><a href="#">Get in Touch!</a></div>
                  <div class="row">
                     <div class="col-sm-12 col-md-3">
                        <aside id="contact-info-widget-2" class="widget contact-info">
                           <div class="contact-info">
                              <ul class="contact-details">
                                 <li><i class="fa fa-envelope"></i> <strong>Email:</strong> <span><a href="mailto:meirezende@hotmail.com">meirezende@hotmail.com</a></span></li>
                                 <li><i class="fa fa-clock-o"></i> <strong>Working Days/Hours:</strong> <span>24/7</span></li>
                              </ul>
                           </div>
                        </aside>
                     </div>
                     <div class="col-sm-12 col-md-3">
                        <aside id="follow-us-widget-3" class="widget follow-us">
                           <h3 class="widget-title">Follow Us</h3>
                           <div class="share-links"><a href="https://www.linkedin.com/in/andremrezende/" target="_blank" data-toggle="tooltip" data-placement="bottom" title="Linkedin" class="share-linkedin">Linkedin</a></div>
                        </aside>
                     </div>
                  </div>
               </div>
            </div>
         </div>
         <div id="blueimp-gallery" class="blueimp-gallery blueimp-gallery-controls" data-start-slideshow="true" data-filter=":even">
            <div class="slides"></div>
            <h3 class="title">&nbsp;</h3>
            <a class="prev"></a> <a class="next"></a> <a class="close"></a> <a class="play-pause"></a>
            <ol class="indicator"></ol>
         </div>
      </div>
      <div class="panel-overlay"></div>
      <div class="filter-overlay"></div>
      <a href="#" id="nav-panel-close" class=""><i class="fa fa-close"></i></a> <!--[if lt IE 9]> <script src="https://www.awslagi.com/wp-content/themes/porto/js/html5shiv.min.js"></script> <script src="https://www.awslagi.com/wp-content/themes/porto/js/respond.min.js"></script> <![endif]--> 
      <noscript>
         <style>.lazyload{display:none}</style>
      </noscript>
      <script data-noptimize="1">window.lazySizesConfig=window.lazySizesConfig||{};window.lazySizesConfig.loadMode=1;</script><script async data-noptimize="1" src='../wp-content/plugins/autoptimize/classes/external/js/lazysizes.minb433.js?ao_version=2.8.4'></script> <script type='text/javascript' id='kk-star-ratings-js-extra'>var kk_star_ratings={"action":"kk-star-ratings","endpoint":"https:\/\/www.awslagi.com\/wp-admin\/admin-ajax.php","nonce":"1856067718"};</script> <script type='text/javascript' id='enlighterjs-js-after'>!function(n,o){"undefined"!=typeof EnlighterJS?(n.EnlighterJSINIT=function(){EnlighterJS.init("pre.EnlighterJSRAW","code.EnlighterJSRAW",{"indent":4,"ampersandCleanup":true,"linehover":true,"rawcodeDbclick":false,"textOverflow":"break","linenumbers":true,"theme":"enlighter","language":"generic","retainCssClasses":false,"collapse":false,"toolbarOuter":"","toolbarTop":"{BTN_RAW}{BTN_COPY}{BTN_WINDOW}{BTN_WEBSITE}","toolbarBottom":""})})():(o&&(o.error||o.log)||function(){})("Error: EnlighterJS resources not loaded yet!")}(window,console);</script> <script type='text/javascript' id='porto-theme-js-extra'>var js_porto_vars={"rtl":"","ajax_url":"https:\/\/www.awslagi.com\/wp-admin\/admin-ajax.php","change_logo":"1","post_zoom":"1","portfolio_zoom":"1","member_zoom":"1","page_zoom":"1","container_width":"1170","grid_gutter_width":"20","show_sticky_header":"1","show_sticky_header_tablet":"1","show_sticky_header_mobile":"1","request_error":"The requested content cannot be loaded.<br\/>Please try again later.","ajax_loader_url":":\/\/www.awslagi.com\/wp-content\/themes\/porto\/images\/ajax-loader@2x.gif","category_ajax":"1","prdctfltr_ajax":"","show_minicart":"0","slider_loop":"1","slider_autoplay":"1","slider_speed":"5000","slider_nav":"1","slider_nav_hover":"1","slider_margin":"","slider_dots":"1","slider_animatein":"","slider_animateout":"","product_thumbs_count":"4","product_zoom":"1","product_zoom_mobile":"1","product_image_popup":"1","zoom_type":"inner","zoom_scroll":"1","zoom_lens_size":"200","zoom_lens_shape":"square","zoom_contain_lens":"1","zoom_lens_border":"1","zoom_border_color":"#888888","zoom_border":"0","screen_lg":"1190"};</script> <script type='text/javascript' id='su-shortcodes-js-extra'>var SUShortcodesL10n={"noPreview":"This shortcode doesn't work in live preview. Please insert it into editor and preview on the site.","magnificPopup":{"close":"Close (Esc)","loading":"Loading...","prev":"Previous (Left arrow key)","next":"Next (Right arrow key)","counter":"%curr% of %total%","error":"Failed to load content. <a href=\"%url%\" target=\"_blank\"><u>Open link<\/u><\/a>"}};</script> <script type="text/javascript">jQuery(document).ready(function(){});</script> <script defer='defer' src="../wp-content/cache/autoptimize/js/autoptimize_736b1861fef28a5b7e4fc506e67a8f74.js"></script>
   </body>
</html>